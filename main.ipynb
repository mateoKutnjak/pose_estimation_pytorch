{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 128, 64, 64]          32,896\n       BatchNorm2d-2          [-1, 128, 64, 64]             256\n              ReLU-3          [-1, 128, 64, 64]               0\n            Conv2d-4          [-1, 128, 64, 64]         147,584\n       BatchNorm2d-5          [-1, 128, 64, 64]             256\n              ReLU-6          [-1, 128, 64, 64]               0\n            Conv2d-7          [-1, 256, 64, 64]          33,024\n       BatchNorm2d-8          [-1, 256, 64, 64]             512\n              ReLU-9          [-1, 256, 64, 64]               0\n   ResidualModule-10          [-1, 256, 64, 64]               0\n           Conv2d-11          [-1, 128, 32, 32]          32,896\n      BatchNorm2d-12          [-1, 128, 32, 32]             256\n             ReLU-13          [-1, 128, 32, 32]               0\n           Conv2d-14          [-1, 128, 32, 32]         147,584\n      BatchNorm2d-15          [-1, 128, 32, 32]             256\n             ReLU-16          [-1, 128, 32, 32]               0\n           Conv2d-17          [-1, 256, 32, 32]          33,024\n      BatchNorm2d-18          [-1, 256, 32, 32]             512\n             ReLU-19          [-1, 256, 32, 32]               0\n   ResidualModule-20          [-1, 256, 32, 32]               0\n           Conv2d-21          [-1, 128, 32, 32]          32,896\n      BatchNorm2d-22          [-1, 128, 32, 32]             256\n             ReLU-23          [-1, 128, 32, 32]               0\n           Conv2d-24          [-1, 128, 32, 32]         147,584\n      BatchNorm2d-25          [-1, 128, 32, 32]             256\n             ReLU-26          [-1, 128, 32, 32]               0\n           Conv2d-27          [-1, 256, 32, 32]          33,024\n      BatchNorm2d-28          [-1, 256, 32, 32]             512\n             ReLU-29          [-1, 256, 32, 32]               0\n   ResidualModule-30          [-1, 256, 32, 32]               0\n           Conv2d-31          [-1, 128, 16, 16]          32,896\n      BatchNorm2d-32          [-1, 128, 16, 16]             256\n             ReLU-33          [-1, 128, 16, 16]               0\n           Conv2d-34          [-1, 128, 16, 16]         147,584\n      BatchNorm2d-35          [-1, 128, 16, 16]             256\n             ReLU-36          [-1, 128, 16, 16]               0\n           Conv2d-37          [-1, 256, 16, 16]          33,024\n      BatchNorm2d-38          [-1, 256, 16, 16]             512\n             ReLU-39          [-1, 256, 16, 16]               0\n   ResidualModule-40          [-1, 256, 16, 16]               0\n           Conv2d-41          [-1, 128, 16, 16]          32,896\n      BatchNorm2d-42          [-1, 128, 16, 16]             256\n             ReLU-43          [-1, 128, 16, 16]               0\n           Conv2d-44          [-1, 128, 16, 16]         147,584\n      BatchNorm2d-45          [-1, 128, 16, 16]             256\n             ReLU-46          [-1, 128, 16, 16]               0\n           Conv2d-47          [-1, 256, 16, 16]          33,024\n      BatchNorm2d-48          [-1, 256, 16, 16]             512\n             ReLU-49          [-1, 256, 16, 16]               0\n   ResidualModule-50          [-1, 256, 16, 16]               0\n           Conv2d-51            [-1, 128, 8, 8]          32,896\n      BatchNorm2d-52            [-1, 128, 8, 8]             256\n             ReLU-53            [-1, 128, 8, 8]               0\n           Conv2d-54            [-1, 128, 8, 8]         147,584\n      BatchNorm2d-55            [-1, 128, 8, 8]             256\n             ReLU-56            [-1, 128, 8, 8]               0\n           Conv2d-57            [-1, 256, 8, 8]          33,024\n      BatchNorm2d-58            [-1, 256, 8, 8]             512\n             ReLU-59            [-1, 256, 8, 8]               0\n   ResidualModule-60            [-1, 256, 8, 8]               0\n           Conv2d-61            [-1, 128, 8, 8]          32,896\n      BatchNorm2d-62            [-1, 128, 8, 8]             256\n             ReLU-63            [-1, 128, 8, 8]               0\n           Conv2d-64            [-1, 128, 8, 8]         147,584\n      BatchNorm2d-65            [-1, 128, 8, 8]             256\n             ReLU-66            [-1, 128, 8, 8]               0\n           Conv2d-67            [-1, 256, 8, 8]          33,024\n      BatchNorm2d-68            [-1, 256, 8, 8]             512\n             ReLU-69            [-1, 256, 8, 8]               0\n   ResidualModule-70            [-1, 256, 8, 8]               0\n           Conv2d-71            [-1, 128, 4, 4]          32,896\n      BatchNorm2d-72            [-1, 128, 4, 4]             256\n             ReLU-73            [-1, 128, 4, 4]               0\n           Conv2d-74            [-1, 128, 4, 4]         147,584\n      BatchNorm2d-75            [-1, 128, 4, 4]             256\n             ReLU-76            [-1, 128, 4, 4]               0\n           Conv2d-77            [-1, 256, 4, 4]          33,024\n      BatchNorm2d-78            [-1, 256, 4, 4]             512\n             ReLU-79            [-1, 256, 4, 4]               0\n   ResidualModule-80            [-1, 256, 4, 4]               0\n           Conv2d-81            [-1, 128, 4, 4]          32,896\n      BatchNorm2d-82            [-1, 128, 4, 4]             256\n             ReLU-83            [-1, 128, 4, 4]               0\n           Conv2d-84            [-1, 128, 4, 4]         147,584\n      BatchNorm2d-85            [-1, 128, 4, 4]             256\n             ReLU-86            [-1, 128, 4, 4]               0\n           Conv2d-87            [-1, 256, 4, 4]          33,024\n      BatchNorm2d-88            [-1, 256, 4, 4]             512\n             ReLU-89            [-1, 256, 4, 4]               0\n   ResidualModule-90            [-1, 256, 4, 4]               0\n           Conv2d-91            [-1, 128, 4, 4]          32,896\n      BatchNorm2d-92            [-1, 128, 4, 4]             256\n             ReLU-93            [-1, 128, 4, 4]               0\n           Conv2d-94            [-1, 128, 4, 4]         147,584\n      BatchNorm2d-95            [-1, 128, 4, 4]             256\n             ReLU-96            [-1, 128, 4, 4]               0\n           Conv2d-97            [-1, 256, 4, 4]          33,024\n      BatchNorm2d-98            [-1, 256, 4, 4]             512\n             ReLU-99            [-1, 256, 4, 4]               0\n  ResidualModule-100            [-1, 256, 4, 4]               0\n          Conv2d-101            [-1, 128, 8, 8]          32,896\n     BatchNorm2d-102            [-1, 128, 8, 8]             256\n            ReLU-103            [-1, 128, 8, 8]               0\n          Conv2d-104            [-1, 128, 8, 8]         147,584\n     BatchNorm2d-105            [-1, 128, 8, 8]             256\n            ReLU-106            [-1, 128, 8, 8]               0\n          Conv2d-107            [-1, 256, 8, 8]          33,024\n     BatchNorm2d-108            [-1, 256, 8, 8]             512\n            ReLU-109            [-1, 256, 8, 8]               0\n  ResidualModule-110            [-1, 256, 8, 8]               0\n          Conv2d-111          [-1, 128, 16, 16]          32,896\n     BatchNorm2d-112          [-1, 128, 16, 16]             256\n            ReLU-113          [-1, 128, 16, 16]               0\n          Conv2d-114          [-1, 128, 16, 16]         147,584\n     BatchNorm2d-115          [-1, 128, 16, 16]             256\n            ReLU-116          [-1, 128, 16, 16]               0\n          Conv2d-117          [-1, 256, 16, 16]          33,024\n     BatchNorm2d-118          [-1, 256, 16, 16]             512\n            ReLU-119          [-1, 256, 16, 16]               0\n  ResidualModule-120          [-1, 256, 16, 16]               0\n          Conv2d-121          [-1, 128, 32, 32]          32,896\n     BatchNorm2d-122          [-1, 128, 32, 32]             256\n            ReLU-123          [-1, 128, 32, 32]               0\n          Conv2d-124          [-1, 128, 32, 32]         147,584\n     BatchNorm2d-125          [-1, 128, 32, 32]             256\n            ReLU-126          [-1, 128, 32, 32]               0\n          Conv2d-127          [-1, 256, 32, 32]          33,024\n     BatchNorm2d-128          [-1, 256, 32, 32]             512\n            ReLU-129          [-1, 256, 32, 32]               0\n  ResidualModule-130          [-1, 256, 32, 32]               0\n================================================================\nTotal params: 2,788,864\nTrainable params: 2,788,864\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 4.00\nForward/backward pass size (MB): 111.78\nParams size (MB): 10.64\nEstimated Total Size (MB): 126.42\n----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/hiddenlayer/graph.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;34m\"\"\"Allows Jupyter notebook to render the graph automatically.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/hiddenlayer/graph.py\u001b[0m in \u001b[0;36mbuild_dot\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mGraphViz\u001b[0m \u001b[0mDigraph\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \"\"\"\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDigraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# Build GraphViz Digraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ],
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": [
       "<hiddenlayer.graph.Graph at 0x152aee518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import hiddenlayer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/mateo/Desktop/FER/SEM4/DIPL/PROJECT_pytorch')\n",
    "\n",
    "import models\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "INPUT_DIM = 1000\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 10\n",
    "\n",
    "x = torch.randn(BATCH_SIZE, INPUT_DIM, device=device)\n",
    "y = torch.randn(BATCH_SIZE, OUTPUT_DIM, device=device)\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(INPUT_DIM, HIDDEN_DIM),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "# ).to(device)\n",
    "#\n",
    "# loss_fn = nn.MSELoss(reduction='sum')\n",
    "# lr = 1e-4\n",
    "#\n",
    "# for i in range(500):\n",
    "#     y_pred = model(x)\n",
    "#\n",
    "#     loss = loss_fn(y_pred, y)\n",
    "#     print(i, loss.item())\n",
    "#\n",
    "#     # COMMENT set gradients to zero\n",
    "#     model.zero_grad()\n",
    "#\n",
    "#     # COMMENT backpropagation\n",
    "#     loss.backward()\n",
    "#\n",
    "#     with torch.no_grad():\n",
    "#         for param in model.parameters():\n",
    "#             param.data -= lr * param.grad\n",
    "#\n",
    "# summary(model, input_size=(1, INPUT_DIM))\n",
    "\n",
    "model = models.HourglassModule(\n",
    "    num_blocks=99,\n",
    "    planes=99,\n",
    "    depth=4\n",
    ").to(device)\n",
    "\n",
    "summary(model, (256, 64, 64))\n",
    "\n",
    "hiddenlayer.build_graph(model, torch.zeros([8, 256, 64, 64]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
